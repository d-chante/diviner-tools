{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec81aa2-986f-487c-82de-d26f6a83df90",
   "metadata": {},
   "source": [
    "# Data Preprocess\n",
    "\n",
    "This Notebook instance provides a procedure to pre-process channel 7 Diviner data collected between January 2010 - September 2023 as part of a goal to replicate the work published in [Unsupervised Learning for Thermophysical Analysis on the Lunar Surface](https://iopscience.iop.org/article/10.3847/PSJ/ab9a52) by Moseley et al. (2020).\n",
    "\n",
    "A particular objective of this pre-processing notebook is to use only a standard computer (CPU, multi-threading) with augmented storage space (~5TB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9d652-7b25-4fc4-8a04-2b6dafb0116d",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a07d66-c24c-45df-a95d-e54e205a74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diviner_tools import DivinerTools\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50a6a8-d81a-4ec8-bcc3-8bf9303eb9b5",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1236a0cb-4cfc-4d78-a339-a69405e04c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parent directory for relevant data products\n",
    "# Note: Esthar has 10TB\n",
    "DATA_DIR = '/esthar/diviner_data'\n",
    "\n",
    "# The name for our database object\n",
    "DB_NAME = 'diviner_data.db'\n",
    "\n",
    "# The filepath for the database\n",
    "DB_FILEPATH = os.path.join(DATA_DIR, \"database\", DB_NAME)\n",
    "\n",
    "# The filepath where tab files will be temporarily saved\n",
    "TAB_DIR = os.path.join(DATA_DIR, \"tab_files\")\n",
    "\n",
    "# Filepath for a text file with all .zip file links\n",
    "ZIP_URLS_FILE = os.path.join(DATA_DIR, \"txt_files\", \"zip_urls.txt\")\n",
    "\n",
    "# Filepath for a text file with .TAB filenames that contain target data\n",
    "USEFUL_TAB_FILE = os.path.join(DATA_DIR, \"txt_files\", \"useful_tabs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea59d42-d1dd-4e0f-ad5a-6c66d77f7569",
   "metadata": {},
   "source": [
    "## Init Diviner Tools\n",
    "\n",
    "diviner_tools is a custom library developed specifically for this task. Upon initialization of the Diviner Tools object, it will create the data directory and database if they don't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a28e4c-abd5-4a9e-9d20-67218c965873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DivinerTools(DATA_DIR, DB_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7fc3e-e9d1-4d57-8d9b-b25807fe42c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract Zip URLs\n",
    "\n",
    "We will extract the URLs for the zip files which contain tab files that contain the RDR LVL1 tables. This can be a somewhat slow process, so we will do this once and then save the urls to a text file. Skip this step if the ZIP_URLS_FILE already exists and has been populated. We expect 717,509 URLs.\n",
    "\n",
    "Each year takes approximately 30-45 seconds, so total time should be around 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db649c2-81be-47b6-9a9d-1cfb5cf17aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_urls = dt.find_all_zip_urls()\n",
    "\n",
    "print(\"Found \" + repr(len(zip_urls)) + \" .zip file urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b33138-5b2a-4d5e-9b4a-97393c9a7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save urls to file\n",
    "append_to_file(ZIP_URLS_FILE, zip_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef546826-3ab3-4c4d-bf6f-ea1941ec4070",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Preprocessing will involve:\n",
    "* Splitting the zip file URLs into batches\n",
    "* For each url, download the .zip file to local directory\n",
    "* Unpack the .zip file\n",
    "* Read the lines from the unpacked .TAB file\n",
    "* Check each line against desired criteria (activity flag, geoemetry flag, etc)\n",
    "* If a line meets the desired criteria, write it to our database\n",
    "* If a .TAB file contains data that was written to the database, save the filename to a textfile\n",
    "* Delete the .TAB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32738b77-32ea-4e97-af35-f194d7c566d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dt.txt_to_list(ZIP_URLS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f81cf0-338c-4e22-8b34-8b8a2a7502bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tmp[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb072c5-a7b3-4ad5-b61c-ca920dd03c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.startDatabaseJobMonitor(DB_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac2739a-754f-4165-ae15-4a19ca5ea77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to complete: datetime.timedelta(seconds=220, microseconds=633145)\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "start_t = datetime.now()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "\n",
    "    futures = [executor.submit(dt.processor, url, TAB_DIR, DB_FILEPATH, USEFUL_TAB_FILE) for url in batch]\n",
    "\n",
    "    # Wait for all futures to complete \n",
    "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "end_t = datetime.now()\n",
    "\n",
    "print(\"Time to complete: \" + repr(end_t - start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0751c9-7b0a-444f-abde-a78add058e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 370 jobs leftttt\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[KJob monitor stopped\n"
     ]
    }
   ],
   "source": [
    "dt.stopDatabaseJobMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723af53-ab34-4882-9b6d-36e444d85777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
