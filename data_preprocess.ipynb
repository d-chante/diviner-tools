{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec81aa2-986f-487c-82de-d26f6a83df90",
   "metadata": {},
   "source": [
    "# Data Preprocess\n",
    "\n",
    "This Notebook instance provides a procedure to pre-process Diviner data. The authors use the Diviner Channel 7 dataset from January 2010 - March 2019. However, since more recent data is available up until 2023, this additional data will be used as well. Note that, LRO changed from a near-circular to an elliptical orbit in December 2011 which changes the effective FOV of point measurements vary with latitude.\n",
    "\n",
    "The totality of the uncompressed data will take up approximately 35TB, while the processed data will be in the neighbourhood of 3TB. In order to minimize the total amount of storage needed, the data will be downloaded and processed in batches. The processed data will be kept, while the unprocessed data will be deleted before processing the next batch.\n",
    "\n",
    "We are interested in the .TAB data files which are stored within .zip files.\n",
    "\n",
    "The steps involved:\n",
    "1. Download Diviner data by month/year\n",
    "2. Filter out the data points with the following parameters:\n",
    " \t* instrument activity == 110 (\"on moon\" orientation, standard nadir observation, nominal instrument mode)\n",
    "\t* calibration == 0 (in-bounds interpolated measurement)\n",
    "\t* geometry flags == 12 (tracking data used to generate geometry information)\n",
    "\t* misc flag == 0 (no misc observations)\n",
    "\t* emission angle < 10 deg (angle between the vector from surface FOV center to DIVINER and the normal vector to the moon's surface)\n",
    "    * only keep data from channel 7\n",
    "3. Sort data into 0.5 deg x 0.5 deg latitude and longitude bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9d652-7b25-4fc4-8a04-2b6dafb0116d",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a07d66-c24c-45df-a95d-e54e205a74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import subprocess\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb6569-4246-4d14-a0bb-4ac6694e03c4",
   "metadata": {},
   "source": [
    "## Setup Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bf81f90-d736-48dd-95fe-6bc504c7d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local directory where the data will be stored\n",
    "# Note: Esthar has 10TB\n",
    "DATA_DIR = '/esthar/diviner_data'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381b446-99e4-4eac-ba7e-892e388cb9b9",
   "metadata": {},
   "source": [
    "## Setup Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f80cb48-597a-4047-bf55-61dd637bf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4159520-403c-41e1-a82c-475172cf6df0",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eb12627-cbd4-40d0-bb34-b837d80d0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@brief Returns a list of sub-links on a parent page.\n",
    "\n",
    "@param parent_url The url page that is being searched.\n",
    "@param pattern A regex pattern if required to filter the url list.\n",
    "\n",
    "@return A list of sub-links on the page.\n",
    "'''\n",
    "def get_sub_urls(parent_url, pattern=None):\n",
    "\n",
    "    # Send a GET request to get page elements\n",
    "    response = requests.get(parent_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract sub-urls\n",
    "    sub_urls = [urljoin(parent_url, link.get(\"href\")) for link in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "    # Filter the list using regex if a pattern is specified\n",
    "    if pattern:\n",
    "        sub_urls = [url for url in sub_urls if re.compile(pattern).match(url)]\n",
    "\n",
    "    return sub_urls\n",
    "    \n",
    "\n",
    "'''\n",
    "@brief Crawls through urls on a page using multithreading\n",
    "\n",
    "@param input_urls The parent urls to search\n",
    "@param pattern Optional regex pattern to match url against \n",
    "'''\n",
    "def multithread_crawl(input_urls, pattern=None):\n",
    "    \n",
    "    # Use multi-threading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        if pattern:\n",
    "            target_urls_list = list(executor.map(lambda target: get_sub_urls(target, pattern), input_urls))\n",
    "        else:\n",
    "            target_urls_list = list(executor.map(lambda target: get_sub_urls(target, target), input_urls))\n",
    "\n",
    "    # Collapse into single list\n",
    "    target_urls = [url for sublist in target_urls_list for url in sublist] \n",
    "\n",
    "    return target_urls\n",
    "    \n",
    "    \n",
    "'''\n",
    "@brief Walks through the RDR V1 parent links to find all\n",
    "       zip file urls\n",
    "'''\n",
    "def find_all_zip_urls(target_year=None):\n",
    "\n",
    "    # lroldr_1001 contains data from 2009 - 2016\n",
    "    # lroldr_1002 contains data from 2017 - 2023\n",
    "    parent_urls = [\n",
    "        'https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1001/data/',\n",
    "        'https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1002/data/']\n",
    "\n",
    "    # Regex pattern will filter URLs for years\n",
    "    # If a year is specified, the search is only for that year\n",
    "    # Otherwise it is for all years 2010-2023\n",
    "    if target_year:\n",
    "        pattern = r'.*/{0}/'.format(target_year)\n",
    "    else:\n",
    "        pattern = r'.*/20[1-2]\\d/$'\n",
    "        \n",
    "    # Generate list of year urls\n",
    "    year_urls = get_sub_urls(parent_urls[0], pattern) + get_sub_urls(parent_urls[1], pattern)\n",
    "    \n",
    "    # Search for month urls\n",
    "    month_urls = multithread_crawl(year_urls)\n",
    "\n",
    "    # Search for day urls\n",
    "    day_urls = multithread_crawl(month_urls)\n",
    "\n",
    "    # Search for zip urls\n",
    "    zip_urls = multithread_crawl(day_urls, r'.+\\.zip$')\n",
    "\n",
    "    return zip_urls\n",
    "        \n",
    "\n",
    "'''\n",
    "@brief Given a link to a .zip file, this function will\n",
    "       download, unpack the .zip file, then delete\n",
    "       the original .zip file to minimize storage used\n",
    "       \n",
    "@param local_dir The local directory to save to\n",
    "@param zip_url The url to the target .zip file\n",
    "'''\n",
    "def download_unpack_delete(dest_dir, src_url):\n",
    "\n",
    "    # Verify the destination directory exists\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Extract filename\n",
    "    filename = os.path.join(dest_dir, src_url.split(\"/\")[-1])\n",
    "\n",
    "    # Download the zip file\n",
    "    response = requests.get(src_url)\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_dir)\n",
    "\n",
    "    # Delete original .zip file\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45db03-8670-45a3-b46f-399049358efd",
   "metadata": {},
   "source": [
    "## 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a672c6-4d52-4aca-99e1-6921bb31871c",
   "metadata": {},
   "source": [
    "First, find all the .zip file urls for the year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75868779-2471-40ea-bdaf-3dd032845339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51483 .zip file urls\n"
     ]
    }
   ],
   "source": [
    "zip_urls = find_all_zip_urls('2010')\n",
    "\n",
    "print(\"Found \" + repr(len(zip_urls)) + \" .zip file urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48454b70-9cac-4309-b580-a700ccc3f7ce",
   "metadata": {},
   "source": [
    "Download the zip files, unpack the .TAB file, and then delete the original zip file. Note that, each .TAB file seems to be about 289M. If we downloaded all 51,483 .TAB files from 2010 alone, that would require just over 14TB of storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bbda0f5-bc4e-408b-9485-aa2e319d2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_unpack_delete(DATA_DIR, zip_urls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a55b9-4aa5-4ff4-ab64-a828850fae22",
   "metadata": {},
   "source": [
    "Insert the .TAB data into the local database, then delete the original .TAB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ecd23-b214-4e6e-b3f9-061f902331fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405592c5-27ca-4a4f-8e82-d5406ed847d4",
   "metadata": {},
   "source": [
    "Delete data points that don't meet the parameter criteriae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4890949d-eb35-435a-8e2f-ee5e64cfa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85f864-0ea5-4e7c-a340-bbde38fe20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
