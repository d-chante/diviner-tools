{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec81aa2-986f-487c-82de-d26f6a83df90",
   "metadata": {},
   "source": [
    "# Data Preprocess\n",
    "\n",
    "This Notebook instance provides a procedure to pre-process Diviner data. The authors use the Diviner Channel 7 dataset from January 2010 - March 2019. However, since more recent data is available up until 2023, this additional data will be used as well. Note that, LRO changed from a near-circular to an elliptical orbit in December 2011 which changes the effective FOV of point measurements vary with latitude.\n",
    "\n",
    "The totality of the uncompressed data will take up approximately 35TB, while the processed data will be in the neighbourhood of 3TB. In order to minimize the total amount of storage needed, the data will be downloaded and processed in batches. The processed data will be kept, while the unprocessed data will be deleted before processing the next batch.\n",
    "\n",
    "We are interested in the .TAB data files which are stored within .zip files.\n",
    "\n",
    "The steps involved:\n",
    "1. Download Diviner data by month/year\n",
    "2. Filter out the data points with the following parameters:\n",
    " \t* instrument activity == 110 (\"on moon\" orientation, standard nadir observation, nominal instrument mode)\n",
    "\t* calibration == 0 (in-bounds interpolated measurement)\n",
    "\t* geometry flags == 12 (tracking data used to generate geometry information)\n",
    "\t* misc flag == 0 (no misc observations)\n",
    "\t* emission angle < 10 deg (angle between the vector from surface FOV center to DIVINER and the normal vector to the moon's surface)\n",
    "    * only keep data from channel 7\n",
    "3. Sort data into 0.5 deg x 0.5 deg latitude and longitude bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9d652-7b25-4fc4-8a04-2b6dafb0116d",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a07d66-c24c-45df-a95d-e54e205a74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import subprocess\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50a6a8-d81a-4ec8-bcc3-8bf9303eb9b5",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1236a0cb-4cfc-4d78-a339-a69405e04c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local directory where the data will be stored\n",
    "# Note: Esthar has 10TB\n",
    "DATA_DIR = '/esthar/diviner_data'\n",
    "\n",
    "# The name for our database object\n",
    "DB_NAME = 'diviner_data.db'\n",
    "\n",
    "# The filepath for the database\n",
    "DB_FILEPATH = os.path.join(DATA_DIR, DB_NAME)\n",
    "\n",
    "# Enum for data fields\n",
    "FIELD = Enum(\"FIELD\",\n",
    "    [\"DATE\", \"UTC\", \"JDATE\", \"ORBIT\", \"SUNDIST\",\n",
    "    \"SUNLAT\", \"SUNLON\", \"SCLK\", \"SCLAT\", \"SCLON\",\n",
    "    \"SCRAD\", \"SCALT\", \"EL_CMD\", \"AZ_CMD\", \"AF\",\n",
    "    \"ORIENTLAT\", \"ORIENTATION\", \"C\", \"DET\", \"VLOOKX\",\n",
    "    \"VLOOKY\", \"VLOOKZ\", \"RADIANCE\", \"TB\", \"CLAT\",\n",
    "    \"CLON\", \"CEMIS\", \"CSUNZEN\", \"CSUNAZI\", \"CLOCTIME\",\n",
    "    \"QCA\", \"QGE\", \"QMI\"], \n",
    "    start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb6569-4246-4d14-a0bb-4ac6694e03c4",
   "metadata": {},
   "source": [
    "## Setup Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf81f90-d736-48dd-95fe-6bc504c7d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381b446-99e4-4eac-ba7e-892e388cb9b9",
   "metadata": {},
   "source": [
    "## Setup Local Database\n",
    "\n",
    "Since there is a large volume of Diviner data, using a database format may be the most efficient way to manage and access it once downloaded. The table definitions are based on the formatting of the Diviner data. \n",
    "\n",
    "Page 29 of the Diviner RDR Level 1 Sofware Interface Specification describes the RDR data fields: https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1001/document/diviner_rdr_sis.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80cb48-597a-4047-bf55-61dd637bf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the database object doesn't already exist, \n",
    "# connecting will create it.\n",
    "db_connection = sqlite3.connect(DB_FILEPATH)\n",
    "\n",
    "# Creating a cursor object allows us to interact\n",
    "# with the database object through SQL commands\n",
    "db_cursor = db_connection.cursor()\n",
    "\n",
    "# Creating the table schema based on the RDR SIS\n",
    "rdr_lvl1_schema = '''\n",
    "    CREATE TABLE IF NOT EXISTS rdr_lvl1_data (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        date TEXT,\n",
    "        utc TEXT,\n",
    "        jdate REAL,\n",
    "        orbit INTEGER,\n",
    "        sundist REAL,\n",
    "        sunlat REAL,\n",
    "        sunlon REAL,\n",
    "        sclk REAL,\n",
    "        sclat REAL,\n",
    "        sclon REAL,\n",
    "        scrad REAL,\n",
    "        scalt REAL,\n",
    "        el_cmd REAL,\n",
    "        az_cmd REAL,\n",
    "        af INTEGER,\n",
    "        vert_lat REAL,\n",
    "        vert_lon REAL,\n",
    "        c INTEGER,\n",
    "        det INTEGER,\n",
    "        vlookx REAL,\n",
    "        vlooky REAL,\n",
    "        vlookz REAL,\n",
    "        radiance REAL,\n",
    "        tb REAL,\n",
    "        clat REAL,\n",
    "        clon REAL,\n",
    "        cemis REAL,\n",
    "        csunzen REAL,\n",
    "        csunazi REAL,\n",
    "        cloctime REAL,\n",
    "        qca INTEGER,\n",
    "        qge INTEGER,\n",
    "        qmi INTEGER\n",
    "    );\n",
    "'''\n",
    "\n",
    "# Execute the SQL to define the table schema\n",
    "db_cursor.execute(rdr_lvl1_schema)\n",
    "\n",
    "# Then commit and close the connection\n",
    "db_connection.commit()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4159520-403c-41e1-a82c-475172cf6df0",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6eb12627-cbd4-40d0-bb34-b837d80d0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move common functions to a .py file that's imported as a library\n",
    "\n",
    "'''\n",
    "@brief Returns a list of sub-links on a parent page.\n",
    "\n",
    "@param parent_url The url page that is being searched.\n",
    "@param pattern A regex pattern if required to filter the url list.\n",
    "\n",
    "@return A list of sub-links on the page.\n",
    "'''\n",
    "def get_sub_urls(parent_url, pattern=None):\n",
    "\n",
    "    # Send a GET request to get page elements\n",
    "    response = requests.get(parent_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract sub-urls\n",
    "    sub_urls = [urljoin(parent_url, link.get(\"href\")) for link in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "    # Filter the list using regex if a pattern is specified\n",
    "    if pattern:\n",
    "        sub_urls = [url for url in sub_urls if re.compile(pattern).match(url)]\n",
    "\n",
    "    return sub_urls\n",
    "    \n",
    "\n",
    "'''\n",
    "@brief Crawls through urls on a page using multithreading\n",
    "\n",
    "@param input_urls The parent urls to search\n",
    "@param pattern Optional regex pattern to match url against \n",
    "'''\n",
    "def multithread_crawl(input_urls, pattern=None):\n",
    "    \n",
    "    # Use multi-threading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        if pattern:\n",
    "            target_urls_list = list(executor.map(lambda target: get_sub_urls(target, pattern), input_urls))\n",
    "        else:\n",
    "            target_urls_list = list(executor.map(lambda target: get_sub_urls(target, target), input_urls))\n",
    "\n",
    "    # Collapse into single list\n",
    "    target_urls = [url for sublist in target_urls_list for url in sublist] \n",
    "\n",
    "    return target_urls\n",
    "    \n",
    "    \n",
    "'''\n",
    "@brief Walks through the RDR V1 parent links to find all\n",
    "       zip file urls\n",
    "'''\n",
    "def find_all_zip_urls(target_year=None):\n",
    "\n",
    "    # lroldr_1001 contains data from 2009 - 2016\n",
    "    # lroldr_1002 contains data from 2017 - 2023\n",
    "    parent_urls = [\n",
    "        'https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1001/data/',\n",
    "        'https://pds-geosciences.wustl.edu/lro/lro-l-dlre-4-rdr-v1/lrodlr_1002/data/']\n",
    "\n",
    "    # Regex pattern will filter URLs for years\n",
    "    # If a year is specified, the search is only for that year\n",
    "    # Otherwise it is for all years 2010-2023\n",
    "    if target_year:\n",
    "        pattern = r'.*/{0}/'.format(target_year)\n",
    "    else:\n",
    "        pattern = r'.*/20[1-2]\\d/$'\n",
    "        \n",
    "    # Generate list of year urls\n",
    "    year_urls = get_sub_urls(parent_urls[0], pattern) + get_sub_urls(parent_urls[1], pattern)\n",
    "    \n",
    "    # Search for month urls\n",
    "    month_urls = multithread_crawl(year_urls)\n",
    "\n",
    "    # Search for day urls\n",
    "    day_urls = multithread_crawl(month_urls)\n",
    "\n",
    "    # Search for zip urls\n",
    "    zip_urls = multithread_crawl(day_urls, r'.+\\.zip$')\n",
    "\n",
    "    return zip_urls\n",
    "        \n",
    "\n",
    "'''\n",
    "@brief Given a link to a .zip file, this function will\n",
    "       download, unpack the .zip file, then delete\n",
    "       the original .zip file to minimize storage used\n",
    "       \n",
    "@param local_dir The local directory to save to\n",
    "@param zip_url The url to the target .zip file\n",
    "'''\n",
    "def download_unpack_delete(dest_dir, src_url):\n",
    "\n",
    "    # Verify the destination directory exists\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Extract filename\n",
    "    filename = os.path.join(dest_dir, src_url.split(\"/\")[-1])\n",
    "\n",
    "    # Download the zip file\n",
    "    response = requests.get(src_url)\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_dir)\n",
    "\n",
    "    # Delete original .zip file\n",
    "    os.remove(filename)\n",
    "\n",
    "'''\n",
    "'''\n",
    "def check_params(data):\n",
    "\n",
    "    # Check the data conforms to the params:\n",
    "    #    af == 110\n",
    "    #    c == 7\n",
    "    #    cemis < 10\n",
    "    #    qca == 0\n",
    "    #    qge == 12\n",
    "    #    qmi == 0\n",
    "    if (data[FIELD.AF.value] == \"110\") and (data[FIELD.C.value] == \"7\") and \\\n",
    "        (float(data[FIELD.CEMIS.value]) < 10.0) and (data[FIELD.QCA.value] == \"000\") and \\\n",
    "        (data[FIELD.QGE.value] == \"012\") and (data[FIELD.QMI.value] == \"000\"):\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "'''\n",
    "@brief Adds a Diviner RDR LVL1 data line\n",
    "       into a target database.\n",
    "\n",
    "@param dest_db The pathway to the destination database\n",
    "@param data The text line containing the data entry\n",
    "@param 0 or 1 depending if the data was added or not\n",
    "'''\n",
    "def insert_into_database(dest_db, data):\n",
    "    \n",
    "    # Split the line    \n",
    "    values = data.strip().split(',')\n",
    "\n",
    "    # Remove any whitespaces from the values\n",
    "    values = [val.strip() for val in values]\n",
    "\n",
    "    # Check that the data conforms to desired params\n",
    "    dataok = check_params(values)\n",
    "    \n",
    "    if(dataok):\n",
    "        try:\n",
    "            # Connect to db\n",
    "            db_connection = sqlite3.connect(dest_db)\n",
    "            db_cursor = db_connection.cursor()\n",
    "\n",
    "            # Execute SQL to insert new data to table\n",
    "            db_cursor.execute('''\n",
    "                INSERT INTO rdr_lvl1_data  (\n",
    "                    date, utc, jdate, orbit, sundist, sunlat, sunlon, sclk, sclat, sclon,\n",
    "                    scrad, scalt, el_cmd, az_cmd, af, vert_lat, vert_lon, c, det, vlookx,\n",
    "                    vlooky, vlookz, radiance, tb, clat, clon, cemis, csunzen, csunazi,\n",
    "                    cloctime, qca, qge, qmi\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', \n",
    "                (values[FIELD.DATE.value], values[FIELD.UTC.value], float(values[FIELD.JDATE.value]), \n",
    "                 float(values[FIELD.ORBIT.value]), float(values[FIELD.SUNDIST.value]), float(values[FIELD.SUNLAT.value]),\n",
    "                 float(values[FIELD.SUNLON.value]), float(values[FIELD.SCLK.value]), values[FIELD.SCLAT.value], \n",
    "                 float(values[FIELD.SCLON.value]), float(values[FIELD.SCRAD.value]), float(values[FIELD.SCALT.value]), \n",
    "                 float(values[FIELD.EL_CMD.value]), float(values[FIELD.AZ_CMD.value]), float(values[FIELD.AF.value]), \n",
    "                 float(values[FIELD.ORIENTLAT.value]), float(values[FIELD.ORIENTATION.value]), float(values[FIELD.C.value]), \n",
    "                 int(values[FIELD.DET.value]), float(values[FIELD.VLOOKX.value]), float(values[FIELD.VLOOKY.value]), \n",
    "                 float(values[FIELD.VLOOKZ.value]), float(values[FIELD.RADIANCE.value]), float(values[FIELD.TB.value]), \n",
    "                 float(values[FIELD.CLAT.value]), float(values[FIELD.CLON.value]), float(values[FIELD.CEMIS.value]),\n",
    "                 float(values[FIELD.CSUNZEN.value]), float(values[FIELD.CSUNAZI.value]), float(values[FIELD.CLOCTIME.value]), \n",
    "                 float(values[FIELD.QCA.value]), int(values[FIELD.QGE.value]), int(values[FIELD.QMI.value])))\n",
    "\n",
    "            # Commit and close\n",
    "            db_connection.commit()\n",
    "            db_connection.close()\n",
    "\n",
    "            return 1\n",
    "\n",
    "        except sqlite3.Error as e:\n",
    "            print(\"Error inserting data: \", e)\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "\n",
    "'''\n",
    "@brief Parses .TAB file into lines\n",
    "\n",
    "@param src_tab Source .TAB file\n",
    "\n",
    "@return A list of strings\n",
    "'''\n",
    "def tab_to_lines(src_tab):\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # Open and read .TAB file starting at line 5\n",
    "    with open(src_tab, 'r') as file:\n",
    "        for _ in range(4):\n",
    "            next(file)\n",
    "\n",
    "        # Read each line and remove carriage character\n",
    "        for line in file:\n",
    "            lines.append(line.rstrip('^M'))\n",
    "\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c043d5c-dca9-49c4-9998-1dd3f5d2edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tab_to_lines('/esthar/diviner_data/201001010000_RDR.TAB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e150823-0366-4eab-a2b0-a1213553c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886032\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "368c6a14-1b4c-490d-aab6-c0d177a72e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lines added to database\n"
     ]
    }
   ],
   "source": [
    "count = 0  \n",
    "\n",
    "for line in lines:\n",
    "    count += insert_into_database(DB_FILEPATH, line)\n",
    "\n",
    "print(repr(count) + \" lines added to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1afb1d2f-c357-4c31-b914-105fd35ea411",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tab_to_lines('/esthar/diviner_data/201001010010_RDR.TAB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "270567b3-e70c-4e1e-a48c-4ab160aac307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885843\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "228d179e-d3bc-4605-ba88-6dfc0c260148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lines added to database\n"
     ]
    }
   ],
   "source": [
    "count = 0  \n",
    "\n",
    "for line in lines:\n",
    "    count += insert_into_database(DB_FILEPATH, line)\n",
    "\n",
    "print(repr(count) + \" lines added to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45db03-8670-45a3-b46f-399049358efd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a672c6-4d52-4aca-99e1-6921bb31871c",
   "metadata": {},
   "source": [
    "First, find all the .zip file urls for the year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75868779-2471-40ea-bdaf-3dd032845339",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_urls = find_all_zip_urls('2010')\n",
    "\n",
    "print(\"Found \" + repr(len(zip_urls)) + \" .zip file urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48454b70-9cac-4309-b580-a700ccc3f7ce",
   "metadata": {},
   "source": [
    "Download the zip files, unpack the .TAB file, and then delete the original zip file. Note that, each .TAB file seems to be about 289M. If we downloaded all 51,483 .TAB files from 2010 alone, that would require just over 14TB of storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbda0f5-bc4e-408b-9485-aa2e319d2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_unpack_delete(DATA_DIR, zip_urls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a55b9-4aa5-4ff4-ab64-a828850fae22",
   "metadata": {},
   "source": [
    "Insert the .TAB data into the local database, then delete the original .TAB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ecd23-b214-4e6e-b3f9-061f902331fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405592c5-27ca-4a4f-8e82-d5406ed847d4",
   "metadata": {},
   "source": [
    "Delete data points that don't meet the parameter criteriae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890949d-eb35-435a-8e2f-ee5e64cfa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85f864-0ea5-4e7c-a340-bbde38fe20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
